# ================================================================
# B∆Ø·ªöC 4: HU·∫§N LUY·ªÜN M√î H√åNH ANN (Artificial Neural Network)
# ================================================================
# - Input: file CSV ƒë√£ c√≥ c·ªôt "set" (train / val)
# - Output: Bi·ªÉu ƒë·ªì loss & accuracy theo epoch cho c√°c m√¥ h√¨nh
# ================================================================

import os
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt

# üîß T·ª∞ CH·ªåN BACKEND HI·ªÇN TH·ªä ƒê·ªí TH·ªä
try:
    matplotlib.use("TkAgg")  # m·ªü c·ª≠a s·ªï ri√™ng n·∫øu c√≥ GUI
except Exception:
    matplotlib.use("Agg")    # fallback n·∫øu m√¥i tr∆∞·ªùng kh√¥ng h·ªó tr·ª£

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD

# ·∫®n log TensorFlow cho g·ªçn
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# ==============================
# 1Ô∏è‚É£ ƒê·ªåC D·ªÆ LI·ªÜU
# ==============================
file_path = r"D:\Hiep\GK_AI\thuyet_trinh+Code\code\data\hu_features_encoded_train.csv"  # file ƒë√£ c√≥ c·ªôt set
df = pd.read_csv(file_path)

# C·ªôt ƒë·∫ßu v√†o (Hu Moments)
X_cols = ['Hu1','Hu2','Hu3','Hu4','Hu5','Hu6','Hu7']
# C·ªôt ƒë·∫ßu ra (one-hot)
y_cols = ['class_class_0','class_class_1','class_class_2','class_class_3','class_class_4']

# Chia t·∫≠p train / val
X_train = df[df['set'] == 'train'][X_cols].values
y_train = df[df['set'] == 'train'][y_cols].values
X_val = df[df['set'] == 'val'][X_cols].values
y_val = df[df['set'] == 'val'][y_cols].values

print("‚úÖ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng:")
print(f"   - S·ªë m·∫´u train: {X_train.shape[0]}")
print(f"   - S·ªë m·∫´u val:   {X_val.shape[0]}")

# ==============================
# 2Ô∏è‚É£ X√ÇY D·ª∞NG & HU·∫§N LUY·ªÜN M√î H√åNH
# ==============================
def train_ann(hidden_neurons):
    """Hu·∫•n luy·ªán m√¥ h√¨nh ANN v·ªõi s·ªë n∆°-ron ·∫©n t√πy ch·ªçn"""
    model = Sequential([
        Dense(hidden_neurons, input_dim=7, activation='sigmoid'),
        Dense(5, activation='softmax')
    ])
    
    optimizer = SGD(learning_rate=0.1) # T·ªëc ƒë·ªô h·ªçc
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=200, #s·ªë l·∫ßn l·∫∑p (epochs)
        batch_size=len(X_train),  # Batch Gradient Descent
        verbose=0
    )
    return model, history

# ==============================
# 3Ô∏è‚É£ HU·∫§N LUY·ªÜN V·ªöI 4 C·∫§U H√åNH KH√ÅC NHAU
# ==============================
neurons_list = [7,8,9,10] # S·ªë n∆°-ron ·∫©n kh√°c nhau ƒë·ªÉ th·ª≠
histories = {}

for n in neurons_list:
    print(f"\nüîπ Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi {n} n∆°-ron ·∫©n ...")
    _, hist = train_ann(n)
    histories[n] = hist

# ==============================
# 4Ô∏è‚É£ V·∫º ƒê·ªí TH·ªä H√ÄM L·ªñI (LOSS)
# ==============================
plt.figure(figsize=(10,6))
for n, hist in histories.items():
    plt.plot(hist.history['loss'], label=f"Train loss (hidden={n})")
    plt.plot(hist.history['val_loss'], '--', label=f"Val loss (hidden={n})")

plt.title("Bi·ªÉu ƒë·ªì h√†m l·ªói Train/Validation theo Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss (Categorical Cross-Entropy)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show(block=True)  # üî• ƒë·∫£m b·∫£o ƒë·ªì th·ªã kh√¥ng b·ªã t·∫Øt

# ==============================
# 5Ô∏è‚É£ V·∫º ƒê·ªí TH·ªä ƒê·ªò CH√çNH X√ÅC (ACCURACY)
# ==============================
plt.figure(figsize=(10,6))
for n, hist in histories.items():
    plt.plot(hist.history['accuracy'], label=f"Train acc (hidden={n})")
    plt.plot(hist.history['val_accuracy'], '--', label=f"Val acc (hidden={n})")

plt.title("ƒê·ªô ch√≠nh x√°c Train/Validation theo Epoch")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show(block=True)  # üî• gi·ªØ c·ª≠a s·ªï hi·ªÉn th·ªã ƒë·∫øn khi b·∫°n ƒë√≥ng

# ==============================
# 6Ô∏è‚É£ G·ª¢I √ù NH·∫¨N X√âT (cho b√°o c√°o)
# ==============================
print("\nüìä G·ª¢I √ù NH·∫¨N X√âT:")
print("- N·∫øu train_loss v√† val_loss c√πng gi·∫£m d·∫ßn ‚Üí m√¥ h√¨nh h·ªçc t·ªët, kh√¥ng b·ªã qu√° kh·ªõp.")
print("- N·∫øu train_loss gi·∫£m nh∆∞ng val_loss tƒÉng ‚Üí m√¥ h√¨nh c√≥ th·ªÉ b·ªã overfitting.")
print("- N·∫øu loss dao ƒë·ªông m·∫°nh ‚Üí th·ª≠ gi·∫£m learning_rate (v√≠ d·ª• 0.05 ho·∫∑c 0.01).")
print("- So s√°nh 4 m√¥ h√¨nh, ch·ªçn s·ªë n∆°-ron ·∫©n cho val_loss nh·ªè v√† val_accuracy cao nh·∫•t.")
